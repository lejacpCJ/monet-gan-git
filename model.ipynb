{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6281e44-678f-4e98-b45a-07df8813a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model definitions for CycleGAN-style training.\n",
    "\n",
    "Contain:\n",
    "- InstanceNorm, ReflectionPadding2D\n",
    "- ResNet generator builder\n",
    "- PatchGAN discriminator builder\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class InstanceNorm(layers.Layer):\n",
    "    def __init__(self, epsilon=1e-5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channel_dim = int(input_shape[-1])\n",
    "        self.gamma = self.add_weight(\n",
    "            name=\"gamma\", shape=(channel_dim,), initializer=\"ones\", trainable=True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\", shape=(channel_dim,), initializer=\"zeros\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        mean, var = tf.nn.moments(x, axes=[1,2], keepdims=True)\n",
    "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
    "        return x_norm * self.gamma + self.beta\n",
    "\n",
    "\n",
    "class ReflectionPadding2D(layers.Layer):\n",
    "    def __init__(self, padding=(1,1), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if isinstance(padding, int):\n",
    "            self.padding = ((padding, padding),(padding,padding))\n",
    "        elif isinstance(padding, (list,tuple)) and len(padding)==2 and isinstance(padding[0], int):\n",
    "            self.padding = ((padding[0], padding[0]), (padding[1], padding[1]))\n",
    "        else:\n",
    "            self.padding = padding\n",
    "\n",
    "    def call(self, x):\n",
    "        pad_top, pad_bottom = self.padding[0]\n",
    "        pad_left, pad_right = self.padding[1]\n",
    "        paddings = [[0,0],[pad_top, pad_bottom],[pad_left, pad_right],[0,0]]\n",
    "        return tf.pad(x, paddings, mode=\"REFLECT\")\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"padding\": self.padding})\n",
    "        return cfg\n",
    "\n",
    "\n",
    "def residual_block(x_in, filters=256):\n",
    "    x = ReflectionPadding2D(padding=1)(x_in)\n",
    "    x = layers.Conv2D(filters, 3, padding=\"valid\", use_bias=False)(x)\n",
    "    x = InstanceNorm()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = ReflectionPadding2D(padding=1)(x)\n",
    "    x = layers.Conv2D(filters, 3, padding=\"valid\", use_bias=False)(x)\n",
    "    x = InstanceNorm()(x)\n",
    "    return layers.Add()([x_in, x])\n",
    "\n",
    "\n",
    "def build_generator_resnet(input_shape=(256,256,3), n_res_blocks=9):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = ReflectionPadding2D(padding=3)(inputs)\n",
    "    x = layers.Conv2D(64, 7, padding='valid', use_bias=False)(x)\n",
    "    x = InstanceNorm()(x); x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = InstanceNorm()(x); x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = InstanceNorm()(x); x = layers.ReLU()(x)\n",
    "\n",
    "    for _ in range(n_res_blocks):\n",
    "        x = residual_block(x, filters=256)\n",
    "\n",
    "    x = layers.UpSampling2D(size=2, interpolation='bilinear')(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', use_bias=False)(x)\n",
    "    x = InstanceNorm()(x); x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.UpSampling2D(size=2, interpolation='bilinear')(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', use_bias=False)(x)\n",
    "    x = InstanceNorm()(x); x = layers.ReLU()(x)\n",
    "\n",
    "    x = ReflectionPadding2D(padding=3)(x)\n",
    "    x = layers.Conv2D(3, 7, padding='valid')(x)\n",
    "    outputs = layers.Activation('tanh')(x)\n",
    "    return tf.keras.Model(inputs, outputs, name='resnet_generator')\n",
    "\n",
    "\n",
    "def build_patchgan_discriminator(input_shape=(256,256,3), n_filters=64):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(n_filters, 4, strides=2, padding='same')(inp)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf = n_filters * 2\n",
    "    x = layers.Conv2D(nf, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = InstanceNorm()(x); x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf *= 2\n",
    "    x = layers.Conv2D(nf, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = InstanceNorm()(x); x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf *= 2\n",
    "    x = layers.Conv2D(nf, 4, strides=1, padding='same', use_bias=False)(x)\n",
    "    x = InstanceNorm()(x); x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(1, 4, strides=1, padding='same')(x)\n",
    "    return tf.keras.Model(inp, x, name='patchgan_discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032fc44-594b-4a2b-879b-6b52c868be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset utilities: TFRecord parsing and dataset builder\n",
    "\"\"\"\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "IMG_KEY = \"image\"\n",
    "\n",
    "def parse_tf_example(serialized, image_key=IMG_KEY):\n",
    "    features = {image_key: tf.io.FixedLenFeature([], tf.string)}\n",
    "    parsed = tf.io.parse_single_example(serialized, features)\n",
    "    return parsed[image_key]\n",
    "\n",
    "def parse_and_preprocess(serialized, input_size=256, image_key=IMG_KEY):\n",
    "    features = {image_key: tf.io.FixedLenFeature([], tf.string)}\n",
    "    parsed = tf.io.parse_single_example(serialized, features)\n",
    "    img = tf.image.decode_image(parsed[image_key], channels=3, expand_animations=False)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
    "    img = tf.image.resize(img, [input_size, input_size], method=tf.image.ResizeMethod.BILINEAR)\n",
    "    img = img * 2.0 - 1.0  # to [-1,1]\n",
    "    return img\n",
    "\n",
    "def make_image_dataset(tfrecords, batch_size=1, input_size=256, shuffle=True, image_key=IMG_KEY, repeat=True):\n",
    "    files = []\n",
    "    for p in tfrecords:\n",
    "        files += sorted(tf.io.gfile.glob(p))\n",
    "    if not files:\n",
    "        raise ValueError(f\"No TFRecord files found for patterns: {tfrecords}\")\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(8192)\n",
    "    ds = ds.map(lambda x: parse_and_preprocess(x, input_size=input_size, image_key=image_key), num_parallel_calls=AUTOTUNE)\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def count_tfrecord_examples(pattern):\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    total = 0\n",
    "    for f in files:\n",
    "        for _ in tf.data.TFRecordDataset(f):\n",
    "            total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76789213-8ffb-4f75-908a-7787658304ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Misc utilities: denormalization, sampling, warm TFRecord writer\n",
    "\"\"\"\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "def denorm_img_tensor(x):\n",
    "    \"\"\"Tensor in [-1,1] -> uint8 numpy array\"\"\"\n",
    "    x = (x + 1.0) * 127.5\n",
    "    x = tf.clip_by_value(x, 0.0, 255.0)\n",
    "    return tf.cast(x, tf.uint8).numpy()\n",
    "\n",
    "def sample_tfrecords(patterns, sample_n=16, out_dir=\"samples\", image_key=\"image\"):\n",
    "    files = []\n",
    "    for p in patterns:\n",
    "        files += sorted(glob.glob(p))\n",
    "    if not files:\n",
    "        raise ValueError(\"No TFRecord files found for patterns: \" + \",\".join(patterns))\n",
    "    ds = tf.data.TFRecordDataset(files)\n",
    "    ds = ds.map(lambda x: tf.io.parse_single_example(x, {image_key: tf.io.FixedLenFeature([], tf.string)})[image_key])\n",
    "    ds = ds.take(sample_n)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    idx = 0\n",
    "    for b in ds:\n",
    "        img_bytes = b.numpy()\n",
    "        p = Path(out_dir) / f\"sample_{idx:03d}.jpg\"\n",
    "        p.write_bytes(img_bytes)\n",
    "        idx += 1\n",
    "    return idx\n",
    "\n",
    "def write_warm_tfrecord_from_csv(feature_csv, photos_root, out_tfrecord, top_pct=10, image_key=\"image\"):\n",
    "    df = pd.read_csv(feature_csv)\n",
    "    if 'min_monet_nn' not in df.columns:\n",
    "        raise ValueError(\"feature CSV must contain 'min_monet_nn' column\")\n",
    "    df = df.dropna(subset=['min_monet_nn'])\n",
    "    k = max(1, int(len(df) * (top_pct/100.0)))\n",
    "    warm = df.sort_values('min_monet_nn').head(k)\n",
    "    print(f\"Selecting {len(warm)} photos (top {top_pct}%) for warm-start\")\n",
    "    writer = tf.io.TFRecordWriter(out_tfrecord)\n",
    "    def _bytes_feature(b): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[b]))\n",
    "    for _, row in warm.iterrows():\n",
    "        p = Path(row['path'])\n",
    "        if not p.exists():\n",
    "            p = Path(photos_root) / p.name\n",
    "        if not p.exists():\n",
    "            print(\"WARN: missing file:\", row['path'])\n",
    "            continue\n",
    "        img_bytes = p.read_bytes()\n",
    "        feat = {\n",
    "            image_key: _bytes_feature(img_bytes),\n",
    "            \"image/filename\": _bytes_feature(str(p).encode('utf-8')),\n",
    "            \"min_monet_nn\": tf.train.Feature(float_list=tf.train.FloatList(value=[float(row['min_monet_nn'])])),\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feat))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    print(\"Wrote warm-start TFRecord:\", out_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68051696-f638-44bd-84db-d0d89550b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compact training entrypoint for Kaggle / local runs.\n",
    "\n",
    "Usage (from notebook):\n",
    "    from train import train_cyclegan\n",
    "    train_cyclegan(...)\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from models import build_generator_resnet, build_patchgan_discriminator\n",
    "from dataset import make_image_dataset\n",
    "from utils import denorm_img_tensor\n",
    "import numpy as np\n",
    "\n",
    "# Loss helpers\n",
    "def lsgan_d_loss(real_pred, fake_pred):\n",
    "    real_pred = tf.cast(real_pred, tf.float32)\n",
    "    fake_pred = tf.cast(fake_pred, tf.float32)\n",
    "    return tf.reduce_mean((real_pred - 1.0) ** 2) + tf.reduce_mean((fake_pred) ** 2)\n",
    "\n",
    "def lsgan_g_loss(fake_pred):\n",
    "    fake_pred = tf.cast(fake_pred, tf.float32)\n",
    "    return tf.reduce_mean((fake_pred - 1.0) ** 2)\n",
    "\n",
    "def mae(x, y):\n",
    "    x = tf.cast(x, tf.float32); y = tf.cast(y, tf.float32)\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "class ImagePool:\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        out = []\n",
    "        images = list(tf.unstack(images))\n",
    "        for img in images:\n",
    "            if len(self.images) < self.max_size:\n",
    "                self.images.append(img); out.append(img)\n",
    "            else:\n",
    "                if np.random.rand() > 0.5:\n",
    "                    idx = np.random.randint(0, len(self.images))\n",
    "                    tmp = self.images[idx]\n",
    "                    self.images[idx] = img\n",
    "                    out.append(tmp)\n",
    "                else:\n",
    "                    out.append(img)\n",
    "        return tf.stack(out)\n",
    "\n",
    "def train_cyclegan(\n",
    "    monet_tfrecs=(\"data/monet_tfrec/*.tfrec\",),\n",
    "    photo_tfrecs=(\"warm/warm-00000-of-00001.tfrec\",),\n",
    "    image_size=128,\n",
    "    batch_size=1,\n",
    "    n_res_blocks=6,\n",
    "    total_steps=20000,\n",
    "    g_lr=2e-4,\n",
    "    d_lr=2e-4,\n",
    "    lambda_cycle=10.0,\n",
    "    lambda_id=5.0,\n",
    "    sample_dir=\"samples_train\",\n",
    "    log_dir=\"logs/cyclegan\",\n",
    "    ckpt_dir=\"ckpts/cyclegan\",\n",
    "    sample_every=500,\n",
    "    ckpt_every=2000,\n",
    "):\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ds_A = make_image_dataset(monet_tfrecs, batch_size=batch_size, input_size=image_size, shuffle=True)\n",
    "    ds_B = make_image_dataset(photo_tfrecs, batch_size=batch_size, input_size=image_size, shuffle=True)\n",
    "    it_A = iter(ds_A); it_B = iter(ds_B)\n",
    "\n",
    "    G_AB = build_generator_resnet(input_shape=(image_size,image_size,3), n_res_blocks=n_res_blocks)\n",
    "    G_BA = build_generator_resnet(input_shape=(image_size,image_size,3), n_res_blocks=n_res_blocks)\n",
    "    D_A = build_patchgan_discriminator(input_shape=(image_size,image_size,3))\n",
    "    D_B = build_patchgan_discriminator(input_shape=(image_size,image_size,3))\n",
    "\n",
    "    print(\"G_AB params:\", G_AB.count_params())\n",
    "    print(\"G_BA params:\", G_BA.count_params())\n",
    "    print(\"D_A params:\", D_A.count_params())\n",
    "    print(\"D_B params:\", D_B.count_params())\n",
    "\n",
    "    g_opt = tf.keras.optimizers.Adam(g_lr, beta_1=0.5)\n",
    "    d_opt = tf.keras.optimizers.Adam(d_lr, beta_1=0.5)\n",
    "\n",
    "    pool_A = ImagePool(50); pool_B = ImagePool(50)\n",
    "\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    ckpt = tf.train.Checkpoint(G_AB=G_AB, G_BA=G_BA, D_A=D_A, D_B=D_B, g_opt=g_opt, d_opt=d_opt)\n",
    "    manager = tf.train.CheckpointManager(ckpt, directory=ckpt_dir, max_to_keep=5)\n",
    "    latest = manager.latest_checkpoint\n",
    "    if latest:\n",
    "        ckpt.restore(latest)\n",
    "        print(\"Restored from checkpoint:\", latest)\n",
    "    else:\n",
    "        print(\"Training from scratch.\")\n",
    "\n",
    "    def denorm_img(x):\n",
    "        return denorm_img_tensor(x)\n",
    "\n",
    "    def train_step(real_A, real_B):\n",
    "        real_A_f32 = tf.cast(real_A, tf.float32)\n",
    "        real_B_f32 = tf.cast(real_B, tf.float32)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            fake_B = G_AB(real_A, training=True); fake_A = G_BA(real_B, training=True)\n",
    "            cycled_A = G_BA(fake_B, training=True); cycled_B = G_AB(fake_A, training=True)\n",
    "            same_A = G_BA(real_A, training=True); same_B = G_AB(real_B, training=True)\n",
    "            pred_fake_A = D_A(fake_A, training=True); pred_fake_B = D_B(fake_B, training=True)\n",
    "            g_adv_AB = lsgan_g_loss(pred_fake_B); g_adv_BA = lsgan_g_loss(pred_fake_A)\n",
    "            cycle_A_loss = mae(real_A_f32, tf.cast(cycled_A, tf.float32))\n",
    "            cycle_B_loss = mae(real_B_f32, tf.cast(cycled_B, tf.float32))\n",
    "            id_A_loss = mae(real_A_f32, tf.cast(same_A, tf.float32))\n",
    "            id_B_loss = mae(real_B_f32, tf.cast(same_B, tf.float32))\n",
    "            g_loss_AB = g_adv_AB + lambda_cycle*(cycle_A_loss + cycle_B_loss)/2.0 + lambda_id*id_B_loss\n",
    "            g_loss_BA = g_adv_BA + lambda_cycle*(cycle_A_loss + cycle_B_loss)/2.0 + lambda_id*id_A_loss\n",
    "            total_g_loss = g_loss_AB + g_loss_BA\n",
    "\n",
    "        g_vars = G_AB.trainable_variables + G_BA.trainable_variables\n",
    "        g_grads = tape.gradient(total_g_loss, g_vars)\n",
    "        g_grads_vars = [(g, v) for g, v in zip(g_grads, g_vars) if g is not None]\n",
    "        g_opt.apply_gradients(g_grads_vars)\n",
    "\n",
    "        fake_A_pool = pool_A.query(fake_A); fake_B_pool = pool_B.query(fake_B)\n",
    "        with tf.GradientTape() as tape_d:\n",
    "            pred_real_A = D_A(real_A, training=True); pred_real_B = D_B(real_B, training=True)\n",
    "            pred_fake_A_pool = D_A(fake_A_pool, training=True); pred_fake_B_pool = D_B(fake_B_pool, training=True)\n",
    "            d_A_loss = lsgan_d_loss(pred_real_A, pred_fake_A_pool); d_B_loss = lsgan_d_loss(pred_real_B, pred_fake_B_pool)\n",
    "            d_loss = d_A_loss + d_B_loss\n",
    "        d_vars = D_A.trainable_variables + D_B.trainable_variables\n",
    "        d_grads = tape_d.gradient(d_loss, d_vars)\n",
    "        d_grads_vars = [(g, v) for g, v in zip(d_grads, d_vars) if g is not None]\n",
    "        d_opt.apply_gradients(d_grads_vars)\n",
    "\n",
    "        return {\"g_loss\": total_g_loss, \"d_loss\": d_loss, \"g_adv_AB\": g_adv_AB, \"g_adv_BA\": g_adv_BA,\n",
    "                \"cycle_A\": cycle_A_loss, \"cycle_B\": cycle_B_loss, \"id_A\": id_A_loss, \"id_B\": id_B_loss}\n",
    "\n",
    "    start_time = time.time()\n",
    "    for step in range(1, total_steps+1):\n",
    "        real_A = next(it_A); real_B = next(it_B)\n",
    "        metrics = train_step(real_A, real_B)\n",
    "        if step % 50 == 0 or step == 1:\n",
    "            print(f\"[{step:06d}/{total_steps}] g={float(metrics['g_loss']):.4f} d={float(metrics['d_loss']):.4f} (time={(time.time()-start_time)/60.0:.1f} min)\")\n",
    "        if step % 50 == 0:\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"g_loss\", metrics[\"g_loss\"], step=step)\n",
    "                tf.summary.scalar(\"d_loss\", metrics[\"d_loss\"], step=step)\n",
    "        if step % sample_every == 0 or step == 1:\n",
    "            a = real_A[0:1]; b = real_B[0:1]\n",
    "            fake_B = G_AB(a, training=False); fake_A = G_BA(b, training=False)\n",
    "            Image = __import__(\"PIL.Image\").Image\n",
    "            grid = tf.concat([denorm_img(a[0]), denorm_img(fake_B[0]), denorm_img(G_BA(fake_B, training=False)[0])], axis=1)\n",
    "            Image.fromarray(grid.numpy()).save(os.path.join(sample_dir, f\"step_{step:06d}_AtoB.png\"))\n",
    "        if step % ckpt_every == 0 or step == total_steps:\n",
    "            ckpt_path = manager.save(); print(\"Checkpoint saved at:\", ckpt_path)\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f3c35-37fb-43d0-9d87-0faa7957ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inference utilities: restore checkpoint and generate Monet-style images.\n",
    "\"\"\"\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from models import build_generator_resnet, build_patchgan_discriminator\n",
    "from dataset import make_image_dataset\n",
    "from utils import denorm_img_tensor\n",
    "\n",
    "def load_cyclegan_for_inference(ckpt_dir=\"ckpts/cyclegan\", image_size=128, n_res_blocks=6):\n",
    "    G_AB = build_generator_resnet(input_shape=(image_size,image_size,3), n_res_blocks=n_res_blocks)\n",
    "    G_BA = build_generator_resnet(input_shape=(image_size,image_size,3), n_res_blocks=n_res_blocks)\n",
    "    D_A = build_patchgan_discriminator(input_shape=(image_size,image_size,3))\n",
    "    D_B = build_patchgan_discriminator(input_shape=(image_size,image_size,3))\n",
    "    g_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    d_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    ckpt = tf.train.Checkpoint(G_AB=G_AB, G_BA=G_BA, D_A=D_A, D_B=D_B, g_opt=g_opt, d_opt=d_opt)\n",
    "    manager = tf.train.CheckpointManager(ckpt, directory=ckpt_dir, max_to_keep=5)\n",
    "    latest = manager.latest_checkpoint\n",
    "    if latest is None:\n",
    "        raise RuntimeError(f\"No checkpoint found in {ckpt_dir}\")\n",
    "    ckpt.restore(latest).expect_partial()\n",
    "    print(\"Restored checkpoint:\", latest)\n",
    "    return G_BA  # photo -> Monet\n",
    "\n",
    "def denorm_img(x):\n",
    "    x = (x + 1.0) * 127.5\n",
    "    x = tf.clip_by_value(x, 0.0, 255.0)\n",
    "    return tf.cast(x, tf.uint8).numpy()\n",
    "\n",
    "def generate_monet_from_tfrecords(photo_tfrecs=(\"data/photo_tfrec/*.tfrec\",), ckpt_dir=\"ckpts/cyclegan\",\n",
    "                                  out_dir=\"generated_monet_from_tfrec\", image_size=128, n_res_blocks=6,\n",
    "                                  batch_size=1, max_batches=None):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    G_photo2monet = load_cyclegan_for_inference(ckpt_dir=ckpt_dir, image_size=image_size, n_res_blocks=n_res_blocks)\n",
    "    ds_ph = make_image_dataset(photo_tfrecs, batch_size=batch_size, input_size=image_size, shuffle=False, repeat=False)\n",
    "    idx = 0\n",
    "    for batch_idx, real_B in enumerate(ds_ph):\n",
    "        fake_A = G_photo2monet(real_B, training=False)\n",
    "        for i in range(fake_A.shape[0]):\n",
    "            img = denorm_img(fake_A[i])\n",
    "            Image.fromarray(img).save(os.path.join(out_dir, f\"monet_{idx:06d}.png\"))\n",
    "            idx += 1\n",
    "        if (max_batches is not None) and (batch_idx + 1 >= max_batches):\n",
    "            break\n",
    "    print(f\"Saved {idx} Monet-style images to {out_dir}\")\n",
    "\n",
    "def generate_monet_from_folder(input_dir=\"data/photo_jpg\", ckpt_dir=\"ckpts/cyclegan\", out_dir=\"generated_monet_from_folder\",\n",
    "                               image_size=128, n_res_blocks=6):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    G = load_cyclegan_for_inference(ckpt_dir=ckpt_dir, image_size=image_size, n_res_blocks=n_res_blocks)\n",
    "    exts = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n",
    "    paths = [os.path.join(input_dir,f) for f in sorted(os.listdir(input_dir)) if f.lower().endswith(exts)]\n",
    "    for idx, p in enumerate(paths):\n",
    "        img_bytes = tf.io.read_file(p)\n",
    "        img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [image_size, image_size])\n",
    "        img = img * 2.0 - 1.0\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        fake = G(img, training=False)\n",
    "        out = denorm_img(fake[0])\n",
    "        Image.fromarray(out).save(os.path.join(out_dir, f\"monet_{idx:06d}.png\"))\n",
    "    print(f\"Processed {len(paths)} images; outputs in {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92a581-be70-406c-8a61-8c49c5e94167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle inference cell: build model, restore checkpoint, generate 7k-10k 256x256 images and zip them.\n",
    "import os, math, shutil, glob, time\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "# === Settings: change these to match your Kaggle dataset / checkpoint names ===\n",
    "CKPT_PREFIX = \"/kaggle/input/your-checkpoint-dataset/ckpts/cyclegan/ckpt-10\"   # path prefix, no extension\n",
    "PHOTO_TFRECS = \"/kaggle/input/your-dataset/data/photo_tfrec/*.tfrec\"          # or point to /kaggle/input/.../photo_jpg\n",
    "OUT_DIR = \"/kaggle/working/generated_monet\"                                   # outputs must go to /kaggle/working\n",
    "IMAGES_ZIP = \"/kaggle/working/images\"                                         # will create /kaggle/working/images.zip\n",
    "IMAGE_SIZE = 256      # target size required by competition\n",
    "BATCH_SIZE = 1\n",
    "MAX_IMAGES = 10000    # stop if generated this many\n",
    "MIN_IMAGES = 7000\n",
    "\n",
    "# === Ensure output dir exists ===\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Recreate model builder: import from your models.py if present, or re-define minimal generator builder here ===\n",
    "# If you added models.py to the kernel, do: from models import build_generator_resnet\n",
    "# For brevity, this example assumes you have build_generator_resnet available in the notebook environment.\n",
    "# If not, import or paste your generator builder here.\n",
    "from models import build_generator_resnet  # ensure models.py is in notebook files or working dir\n",
    "\n",
    "# === Build generator at IMAGE_SIZE (works if architecture same) ===\n",
    "# If the checkpoint was trained at 128x128 and you want to run at 256, rebuild with image_size=256 but same architecture.\n",
    "GEN = build_generator_resnet(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), n_res_blocks=6)  # adjust n_res_blocks as trained\n",
    "\n",
    "# Create dummy optimizers only if using Checkpoint structure that included them (they are ignored by expect_partial)\n",
    "g_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "d_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "# Restore checkpoint (prefix without extension). This will use ckpt-10.index/.data-00000-of-00001 automatically.\n",
    "ckpt = tf.train.Checkpoint(G_AB=None, G_BA=GEN, D_A=None, D_B=None, g_opt=g_opt, d_opt=d_opt)\n",
    "# If you trained with G_BA name as photo->monet, restore into that slot (G_BA). Adjust names if different.\n",
    "print(\"Restoring checkpoint:\", CKPT_PREFIX)\n",
    "ckpt.restore(CKPT_PREFIX).expect_partial()\n",
    "print(\"Checkpoint restore finished.\")\n",
    "\n",
    "# === Dataset: either TFRecords or image folder ===\n",
    "def parse_and_preprocess(serialized, input_size=IMAGE_SIZE, image_key=\"image\"):\n",
    "    features = {image_key: tf.io.FixedLenFeature([], tf.string)}\n",
    "    parsed = tf.io.parse_single_example(serialized, features)\n",
    "    img = tf.image.decode_image(parsed[image_key], channels=3, expand_animations=False)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
    "    img = tf.image.resize(img, [input_size, input_size], method=\"bilinear\")\n",
    "    img = img * 2.0 - 1.0\n",
    "    return img\n",
    "\n",
    "def make_image_dataset_from_tfrecs(pattern, batch_size=1, input_size=IMAGE_SIZE):\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No TFRecord files found for pattern: \" + pattern)\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(lambda x: parse_and_preprocess(x, input_size=input_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# If you prefer raw JPGs:\n",
    "def make_image_dataset_from_folder(folder, batch_size=1, input_size=IMAGE_SIZE):\n",
    "    exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")\n",
    "    files = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.lower().endswith(exts)]\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No image files in folder: \" + folder)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    def _load(p):\n",
    "        img = tf.io.read_file(p)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [input_size, input_size])\n",
    "        img = img * 2.0 - 1.0\n",
    "        return img\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# pick dataset source here:\n",
    "use_tfrecs = True\n",
    "if use_tfrecs:\n",
    "    ds = make_image_dataset_from_tfrecs(PHOTO_TFRECS, batch_size=BATCH_SIZE, input_size=IMAGE_SIZE)\n",
    "else:\n",
    "    ds = make_image_dataset_from_folder(\"/kaggle/input/your-dataset/data/photo_jpg\", batch_size=BATCH_SIZE, input_size=IMAGE_SIZE)\n",
    "\n",
    "# === Generation loop ===\n",
    "def denorm_and_save(tensor, out_path):\n",
    "    img = (tensor + 1.0) * 127.5\n",
    "    img = tf.clip_by_value(img, 0, 255)\n",
    "    arr = tf.cast(img, tf.uint8).numpy()\n",
    "    Image.fromarray(arr).save(out_path)\n",
    "\n",
    "count = 0\n",
    "start = time.time()\n",
    "for batch in ds:\n",
    "    fake = GEN(batch, training=False)       # outputs in [-1,1] shape [B,H,W,3]\n",
    "    # if your GEN was trained at 128 and you built at 256, it still produces 256 sized output here.\n",
    "    for i in range(fake.shape[0]):\n",
    "        out_path = os.path.join(OUT_DIR, f\"monet_{count:06d}.png\")\n",
    "        denorm_and_save(fake[i], out_path)\n",
    "        count += 1\n",
    "        if count >= MAX_IMAGES:\n",
    "            break\n",
    "    if count >= MAX_IMAGES:\n",
    "        break\n",
    "print(f\"Generated {count} images in {time.time()-start:.1f}s\")\n",
    "\n",
    "# Validate count\n",
    "if count < MIN_IMAGES:\n",
    "    raise RuntimeError(f\"Generated only {count} images; need at least {MIN_IMAGES} for submission\")\n",
    "\n",
    "# Zip outputs as images.zip at /kaggle/working/images.zip\n",
    "zip_base = \"/kaggle/working/images\"\n",
    "if os.path.exists(zip_base + \".zip\"):\n",
    "    os.remove(zip_base + \".zip\")\n",
    "shutil.make_archive(zip_base, 'zip', OUT_DIR)\n",
    "print(\"Wrote images.zip ->\", zip_base + \".zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_308_tf_210]",
   "language": "python",
   "name": "conda-env-python_308_tf_210-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
